{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Hardy-Weinberg is trinomial with probabilities $(\\theta^2, 2 \\theta (1-\\theta), (1-\\theta)^2)$. So the likelihood function is given by:\n",
    "\n",
    "$$L(\\theta) = {n_1 + n_2 + n_3 \\choose n_1} {n_2 + n_3 \\choose n_2} {n_3 \\choose n_3} \\theta^{2n_1} (2 \\theta (1-\\theta))^{n_2} (1-\\theta)^{2n_3} $$\n",
    "\n",
    "So the log-likelihood is given by:\n",
    "\n",
    "$$\\log L(\\theta) = constant + 2n_1 \\log \\theta + n_2 \\log \\theta + n_2 \\log (1-\\theta) + 2n_3 \\log (1-\\theta) $$\n",
    "\n",
    "Then the MLE of $\\theta$ is given by the solution to the score equation:\n",
    "\n",
    "\\begin{align}\n",
    "S(\\theta) = 0 & = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta) \\\\\n",
    "& = \\frac{2n_1 + n_2}{\\theta} - \\frac{n_2 + 2n_3}{1-\\theta} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Solving for $\\theta$ we have that the MLE is given by:\n",
    "\n",
    "$$\\hat{\\theta} = \\frac{2n_1 + n_2}{2n_1 + 2n_2 + 2n_3}$$\n",
    "\n",
    "And the Fisher information is given by:\n",
    "\n",
    "\\begin{align}\n",
    "I(\\theta) & = - \\frac{\\partial}{\\partial \\theta^2} \\log L(\\theta) \\\\\n",
    "& = \\frac{2n_1 + n_2}{\\theta^2} + \\frac{n_2 + 2n_3}{(1-\\theta)^2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Given $n_1 = 125$, $n_2 = 34$ and $n_3 = 10$ we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_hw(theta: List[float], n1: int, n2: int, n3: int) -> float:\n",
    "    AA = theta ** (2*n1)\n",
    "    Aa = (2 * theta* (1 - theta)) ** n2\n",
    "    aa = (1 - theta) ** (2*n3)\n",
    "    like = comb(n1+n2+n3, n1) * comb(n2+n3, n2) * comb(n3, n3) * AA * Aa * aa\n",
    "    return like / np.max(like)\n",
    "\n",
    "def mle_hw(n1: int, n2: int, n3: int) -> float:\n",
    "    mle = (2*n1 + n2) / (2*n1 + 2*n2 + 2*n3)\n",
    "    return mle\n",
    "\n",
    "def standard_error(theta_hat: float, n1: int, n2: int, n3: int) -> float:\n",
    "    obs_fisher_info = (2*n1 + n2)/theta_hat**2 + (n2 + 2*n3)/(1 - theta_hat)**2\n",
    "    se = 1 / np.sqrt(obs_fisher_info)\n",
    "    return se\n",
    "    \n",
    "\n",
    "def plot_log_likelihood(theta: List[float], likelihood: List[float]):\n",
    "    plt.plot(theta, likelihood)\n",
    "    plt.title('Hardy-Weinberg law likelihood')\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel('Likelihood');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(0, 1, 100)\n",
    "n1 = 125\n",
    "n2 = 34\n",
    "n3 = 10\n",
    "likelihood = likelihood_hw(theta, n1, n2, n3)\n",
    "plot_log_likelihood(theta, likelihood)\n",
    "mle = round(mle_hw(n1, n2, n3), 2)\n",
    "print(f\"MLE = {mle}\")\n",
    "se = round(standard_error(mle, n1, n2, n3), 2)\n",
    "print(f\"standard error = {se}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Comparing the 95% like-likelihood based interval with the Wald interval we see that they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_interval(theta: List[float],\n",
    "                        likelihood: List[float],\n",
    "                        cutoff: float) -> Tuple[float, float]:\n",
    "    # intersection points occur below and above the maximum likelihood estimate\n",
    "    mle_index = np.argmax(likelihood)\n",
    "    interp_below_max = interp1d(likelihood[:mle_index], theta[:mle_index])\n",
    "    interp_above_max = interp1d(likelihood[mle_index:], theta[mle_index :])\n",
    "    lower_int = np.round(interp_below_max(cutoff).flatten()[0], 2)\n",
    "    upper_int = np.round(interp_above_max(cutoff).flatten()[0], 2)\n",
    "    return (lower_int, upper_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0.15 # 95% likelihood interval\n",
    "print(f'95% likelihood interval (c = {c}) for θ is {likelihood_interval(theta, likelihood, c)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'95% Wald confidence interval for θ is {(round(mle - 1.96 * se, 2), round(mle + 1.96 * se, 2))}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Hardy-Weinberg is now binomial with probabilities $(1-(1-\\theta)^2), (1-\\theta)^2)$. So the likelihood function is given by:\n",
    "\n",
    "$$L(\\theta) = {n_1 + n_2 + n_3 \\choose n_1 + n_2} {n_3 \\choose n_3} (\\theta (2-\\theta))^{n_1 + n_2} (1-\\theta)^{2n_3} $$\n",
    "\n",
    "Note that the constant term can be dropped here like above without changing the likelihood. The above steps would then be repeated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
